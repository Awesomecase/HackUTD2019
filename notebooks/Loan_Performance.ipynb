{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## This script will walk through a basic analysis of the Acquisition and Performance data for the Data Science\n",
    "## Challenge (Nov 9th and 10th 2019)\n",
    "##\n",
    "## Here you will find the tools necessary to open, read, and process the data as well as give you some \n",
    "## idea of the types of differing risk factors over the years. The key focus of your your analysis\n",
    "## should be built around the \"Zero_Bal_Cd\" attribute. Look further into the script for more details\n",
    "## on the different values this field can take on.\n",
    "##\n",
    "## In order to run this script you need to download data from this link: (you will have to create an account\n",
    "## on the site but it's free)\n",
    "##    https://loanperformancedata.fanniemae.com/lppub/index.html#Portfolio\n",
    "##\n",
    "## Additional details about these datasets (attribute names, allowable values, definitions, etc:\n",
    "## is available from here:\n",
    "##    https://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html\n",
    "##\n",
    "## Download the data the 3rd Quarter for the years 2004, 2008, 2012, and 2016.\n",
    "##\n",
    "## Unzip the data files into the \"RawData\" directory and then execute this script.\n",
    "##\n",
    "## Make sure you have all the necessary python libraries listed below installed on your system\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## print our current working directory to be sure we're operating in the right place\n",
    "##\n",
    "##os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## create a list of the acquisition data file names\n",
    "##\n",
    "all_Acq_files = glob.glob(os.path.join(\"data/Acquisition*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## print out the list to make sure we've got them all\n",
    "##\n",
    "all_Acq_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## read the contents of each acquisition file into a data frame\n",
    "##\n",
    "df_from_each_file = (pd.read_csv(f,sep =\"|\", index_col=None, header=None) for f in all_Acq_files)\n",
    "df   = pd.concat(df_from_each_file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## The files don't have names for each column so add the columns here\n",
    "##\n",
    "df.rename(columns={\n",
    "                    0: 'Loan_ID',\n",
    "                    1: 'Channel',\n",
    "                    2: 'Seller',\n",
    "                    3: 'Interest_Rate',\n",
    "                    4: 'UPB',\n",
    "                    5: 'Loan_Term',\n",
    "                    6: 'Origination_Date',\n",
    "                    7: 'First_Payment_Date',\n",
    "                    8: 'LTV',\n",
    "                    9: 'CLTV',\n",
    "                    10: 'Num_Borrowers',\n",
    "                    11: 'DTI',\n",
    "                    12: 'Borrower_FICO',\n",
    "                    13: 'First_Time_Buyer',\n",
    "                    14: 'Loan_Purpose',\n",
    "                    15: 'Dwelling_Type',\n",
    "                    16: 'Unit_Count',\n",
    "                    17: 'Occupancy',\n",
    "                    18: 'State',\n",
    "                    19: 'Zip',\n",
    "                    20: 'Insurance%',\n",
    "                    21: 'Product',\n",
    "                    22: 'Co_Borrower_FICO',\n",
    "                    23: 'Mortgage_Insurance_Type',\n",
    "                    24: 'Relocation_Indicator'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Now grab a listing of all the performance files in the RawData directory\n",
    "##\n",
    "all_perf_files = glob.glob(os.path.join( \"RawData/Performance_*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## display a listong of the performance files to make sure the year/quarter aligns\n",
    "## with the acquisition files\n",
    "##\n",
    "all_perf_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## read in the data from each of the performance files and concatenate the\n",
    "## data together into a single dataframe names \"perf_df\" and while we're reading the data in only include\n",
    "## the columns we actually want for this analysis.\n",
    "##\n",
    "df_from_each_file = (pd.read_csv(f,sep =\"|\", index_col=None, header=None\n",
    "                                 ,usecols=[0,1,3,4,5,11,12]\n",
    "                                 , names = ['Loan_ID', 'Period', 'Current_IR','Current_UPB', 'Age',\n",
    "                                            'Mod_Ind','Zero_Bal_Cd']\n",
    "                                 ,dtype = { 'Loan_ID' : np.int64, 'Current_IR' : np.float64, \n",
    "                                           'Current_UPB': np.float64}\n",
    "                                ) for f in all_perf_files)\n",
    "perf_df   = pd.concat(df_from_each_file, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Modify the date field (\"Period\") to be a number for easier manipulation\n",
    "## later on in the script\n",
    "## \n",
    "perf_df['Period']=perf_df['Period'].apply(str).str[6:].apply(int)*100+perf_df['Period'].apply(str).str[:2].apply(int)\n",
    "\n",
    "## Select the latest period in the data frame as we're concerned with the most recent loan status\n",
    "##\n",
    "idx = perf_df.groupby(['Loan_ID'])['Period'].transform(max) == perf_df['Period']\n",
    "\n",
    "## Create a new data frame with just the latest period record\n",
    "## \n",
    "perf_df_new = perf_df[idx].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-489a1f02b4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## about those records as regards our analysis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mperf_df_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero_Bal_Cd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m## Also, some of the loans are missing the UPB (unpaid balance). We can't use that data in building our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perf_df_new' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'perf_df_new' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "## In looking at the FAQ dor the datasets we know that if the zero balance code is null then the loan is current\n",
    "## meaning it's paid up correctly. It's not late, or paid off early, or in default so we don't really care\n",
    "## about those records as regards our analysis.\n",
    "##\n",
    "perf_df_new.Zero_Bal_Cd.fillna(0,inplace=True)\n",
    "\n",
    "## Also, some of the loans are missing the UPB (unpaid balance). We can't use that data in building our model\n",
    "## so we'll just drop those loans from the dataframe\n",
    "##\n",
    "perf_df_new.dropna(inplace=True)\n",
    "\n",
    "## create a mapping of the available zero_balance_code numbers and their meanings\n",
    "##\n",
    "zero_bal_cd_map = {0:'Current',1:'Prepaid',2:'Third Party Sale',3:'Short Sale',\n",
    "                   6:'Repurchase',9:'REO',15:'Note Sale',16:'RPL Loan Sale'}\n",
    "perf_df_new['Zero_Bal_Cd'] = perf_df_new['Zero_Bal_Cd'].map(zero_bal_cd_map).apply(str)\n",
    "\n",
    "## display a listing of the updated performance data \n",
    "##\n",
    "perf_df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-df4a076f9fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## data frame that we'll call \"loan_df\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloan_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mperf_df_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Loan_ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## display the first several rows from the combined dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "## Now that we've cleaned up the acquisition and performance data, merge them into a single integrated\n",
    "## data frame that we'll call \"loan_df\"\n",
    "##\n",
    "loan_df = pd.merge(df,perf_df_new,how='inner',on='Loan_ID')\n",
    "\n",
    "## display the first several rows from the combined dataset\n",
    "##\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53e858c18ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloan_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loan_df' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'loan_df' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "loan_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Assign Defaults for the missing values in the loans dataframe\n",
    "##\n",
    "loan_df.Mortgage_Insurance_Type.fillna(0,inplace=True)\n",
    "loan_df['Insurance%'].fillna(0,inplace=True)\n",
    "loan_df.Num_Borrowers.fillna(1,inplace=True)\n",
    "loan_df.CLTV.fillna(loan_df.LTV,inplace=True)\n",
    "loan_df.drop('Co_Borrower_FICO',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "loan_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Drop any records that still have null values - we don't want to include them in the model / analysis\n",
    "##\n",
    "loan_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## We'll do some analysis against the FICO (credit score) of the borrower\n",
    "## Create several bins based on the FICO score range and add the calculated FICO bin score\n",
    "## to each record in the dataframe\n",
    "##\n",
    "FICO_bins = [0,620,660,700,740,780,850]\n",
    "FICO_labels = ['0-620', '620-660','660-700','700-740','740-780','780+']\n",
    "loan_df['FICO_bins'] = pd.cut(loan_df['Borrower_FICO'],bins=FICO_bins,labels=FICO_labels)\n",
    "\n",
    "Term_bins =[0,180,360]\n",
    "Term_labels =['<=15 Years','<= 30 Years']\n",
    "loan_df['Term_bins'] = pd.cut(loan_df['Loan_Term'],bins=Term_bins,labels=Term_labels)\n",
    "\n",
    "zero_bal_cd_map = {'Current':'Current','Prepaid':'Prepaid','Third Party Sale':'Underperforming','Short Sale':'Underperforming',\n",
    "                   'Repurchase':'Underperforming','REO':'Underperforming','Note Sale':'Underperforming','RPL Loan Sale':'Underperforming'}\n",
    "loan_df['Current_Status'] = loan_df['Zero_Bal_Cd'].map(zero_bal_cd_map).apply(str)\n",
    "\n",
    "loan_df['Origin_Month'],loan_df['Origin_Year'] = loan_df['First_Payment_Date'].str.split('/', 1).str\n",
    "\n",
    "df = loan_df[loan_df['Origin_Year'].isin(['2003','2008','2012','2016'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Build a table showing the current status and total for each status type by year\n",
    "##\n",
    "df.groupby(['Origin_Year','Current_Status']).agg({'Loan_ID':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Create a new dataframe that holds the first 100,000 records\n",
    "##\n",
    "df2 = df.groupby('Origin_Year').head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## display another table showing the total number of each status by year\n",
    "##\n",
    "df2.groupby(['Origin_Year','Current_Status']).agg({'Loan_ID':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Dump some info about the attributes that make up our dataframe\n",
    "##\n",
    "## Out of this list of attributes, which ones (and with what values) correlate the most with prepaid and defaulted\n",
    "## mortages for each of the 4 years? Is there some attribute that remains significant across all four years?\n",
    "## These are the \"risk factors\" we're interested in having you find in the data and display in some visually\n",
    "## interesting way.\n",
    "##\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Write our current data frame out to a file. This will allow us to pick up and continue our\n",
    "## analysis without going through all the previous work to clean and structure\n",
    "## the data correctly.\n",
    "##\n",
    "df2.to_csv('Processed_loans.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Create and display a plot showing the distribution by year for the Borrower FICO score vs\n",
    "## current, underperforming, and prepaid (paid off early) loans\n",
    "##\n",
    "sns.relplot(y='Borrower_FICO', x='Interest_Rate', data=df2\n",
    "            ,hue='Dwelling_Type',\n",
    "            row='Current_Status', col='Origin_Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Rebalance the record set by Dwelling Type into a new data frame (df3). We're doing this because the Single\n",
    "## Family data (individual family homes) is far larger than the other types of homes (multi-family, etc) and we want\n",
    "## a more balanced view of the data. You could go back and do a deeper analysis by home type and look for risk\n",
    "## factors on that basis, or by zip code, number of borrowers, etc...\n",
    "##\n",
    "g = df2.groupby(['Origin_Year','Dwelling_Type'])\n",
    "df3 = g.apply(lambda x: x.sample(g.size().min())).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Redisplay the graph based on the same attributes used above\n",
    "##\n",
    "sns.relplot(y='Borrower_FICO', x='Interest_Rate', data=df3#.query('Current_Status != \"Prepaid\"'), #kind='line', \n",
    "            ,hue='Dwelling_Type',\n",
    "            row='Current_Status', col='Origin_Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}